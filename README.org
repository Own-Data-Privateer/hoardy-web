* What?
=pwebarc= is a suite of tools implementing a Private Web Archive, basically your own private [[https://web.archive.org/][Wayback Machine]].

It consists of

- Required parts:
  - =pwebarc-dumb-dump-server.py=: [[./dumb_server/][A dumb archiving server]], written in [[./dumb_server/pwebarc-dumb-dump-server.py][less than 120 lines of pure Python]] that uses the Python's standard library and nothing else. Have Python installed? You can run it right now, no need to install anything else.
  - =pWebArc.xpi=: [[./extension/][A browser extension]] (on [[https://addons.mozilla.org/en-US/firefox/addon/pwebarc/][addons.mozilla.org]], currently only for Firefox-based browsers, but Chromium/Chrome support is coming eventually) that collects your browsing data (HTTP requests and responses) and sends them to the archiving server as you browse.
- Recommended parts:
  - [[./firefox/][A patch for Firefox]] to allow the above extension to properly collect request POST data. This is not required, but could be useful if you want to archive POST request properly.
- WIP parts:
  - A tool to display, search, manipulate, and deduplicate archive files.
  - A set of tools to convert mitmproxy, WARC, HAR, and PCAP files into the internal format used by =pwebarc= and from the internal format to at least WARC.
  - (eventually) A non-dumb server with data deduplication, timelines, full text search, and etc.
* Why?
So, you wake up remembering something interesting, you try to look it up on Google, you fail, eventually you remember the website you seen it at (or a tool like  [[https://github.com/karlicoss/promnesia][Promnesia]] helps you), you go there to look it up... and discovered it offline/gone/a parked domain.
Not a problem! Have no fear! You go to [[https://web.archive.org/][Wayback Machine]] and look it up there... and discover they only archived an ancient version of it and the thing you wanted is missing there.

Or, say, you read a cool fanfiction on [[https://archiveofourown.org/][AO3]] years ago, you even wrote down the URL, you go back to it wanting to experience it again... and discover the author made it private.

Or, say, there's a web page/app you use (like a banking app), but it lacks some features you want, and in your browser's Network Monitor you can see it uses JSON RPC or some such to fetch its data, and you want those JSONs for yourself (e.g. to compute statistics and supplement the app output with them), but the app in question has no public API and scraping it with a script is non-trivial (e.g. they do complicated JavaScript+multifactor-based auth, try to detect you are actually using a browser, and they ban you immediately if not).

All of these scenarios happen all the time to me. "If it is on the Internet, it is on Internet forever!" they said. "Everything will have a REST API!" they said. *They lied!*

Things vanish from the Internet all the time, [[https://web.archive.org/][Wayback Machine]] is awesome, but

- you need to be online to use it,
- it has no full text search, even though it was promised for decades now (this is probably a privacy feature by this point),
- they remove/hide archived data sometimes under political pressure,
- they only archive the public web and only what can be reached with GET requests.

And, obviously, you wouldn't want it to archive you banking app's output.
* But you could do X instead
** But you could use [[https://github.com/danny0838/webscrapbook][WebScrapBook]] instead
Sure, but

- you will have to manually capture each page you want to save (and if this is what you want you should use that extension instead of this),
- you won't be able to get JSONs fetched by web apps you run with it, it only captures web pages.
** But you could just enable request logging in your browser's Network Monitor and manually save your data as HAR archives from time to time.
Well, yes, but

- you will have to manually enable it for each browser tab,
- opening a link in a new tab will fail to archive the first page as you will not have Network Monitor open there yet, and then
- you will have to check all your tabs for new data all the time and do ~5 clicks per tab to save it, and then
- HARs are JSON, meaning all that binary data gets encoded indirectly, thus making resulting HAR archives very inefficient for long-term storage, even when compressed (TODO on-disk space comparison).

And then you still need something like this suite to look into the generated archives.
** But you could use [[https://github.com/webrecorder/archiveweb.page][archiveweb.page]] instead.
Yes, but

- it's Chromium-only,
- stores data internally in the browser, as similarly inefficient JSONs, and
- it also requires constant user interaction to export the data out.

And then you still need something like this suite to look into the generated archives.
** But you could use [[https://github.com/mitmproxy/mitmproxy][mitmproxy]] instead.
Yes, but

- websites using certificate pinning do not work under it,
- it is rather painful to setup, needing you to install a custom SSL root certificate, and
- websites can detect when you use it and fingerprint you for it or force you to solve CAPTCHAs.

And then you still need something like this suite to look into the generated archives.
** But you could setup SSL keys dumping then use Wireshark to capture your web traffic.
Yes, but

- it is really painful to setup, and then
- it takes a lot of effort to recover HTTP data from the PCAP dumps, and
- PCAP dumps are IP packet-level, thus also inefficient for this use case, and
- PCAP dumps of SSL traffic can not be compressed much.

And then you still need something like this suite to look into the generated archives.
* Meanwhile, this suite of tools
With =pwebarc=, [[./extension/][the extension]] simply collect all the data as you browse, immediately sends it to the archiving sever, and [[./dumb_server/][the dumb archiving server implementation]] simply dumps data it gets to disk, one file per HTTP request+response pair.
You can figure out what to do with it later.

=pwebarc= uses compressed [[https://datatracker.ietf.org/doc/html/rfc8949][CBOR (RFC8949)]] of decoded HTTP data as on-disk representation format, which is actually more efficient than storing raw HTTP request dumps.
After converting all my previous =wget=, =curl=, =mitmproxy=, and HAR archives into this, it is about as efficient as compressed =mitmproxy= dumps, with some (WIP) data-deduplication and xdelta compression between same-URL revisions it is much more efficient.
For me, it uses about *3GiB per year of browsing* on average (~5 years of mostly uninterrupted data collection ATM) but I use things like uBlockOrigin and uMatrix to cut things down, and image boorus and video hosting sites have their own pipelines.
* How to use
** Quickstart
:PROPERTIES:
:CUSTOM_ID: quickstart
:END:
- Download [[./dumb_server/pwebarc-dumb-dump-server.py][the dumb archiving server script]] and run it, it will start saving data into =pwebarc-dump= directory wherever you run it from.
- Install the browser extension [[https://addons.mozilla.org/en-US/firefox/addon/pwebarc/][into your Firefox-based browser]].

You are done.
** Developing the extension
- =git clone= this repository.
- In Firefox, go to =about:debugging#/runtime/this-firefox=, click "Load Temporary Add-on" button, and select [[./extension/manifest.json]].
- Then you might need to go into =about:addons= and enable "Run in Private Windows" for =pWebArc= if your Firefox is running in Private-Windows-only mode.
** Installing an unsigned XPI
- Make sure your browser [[https://wiki.mozilla.org/Add-ons/Extension_Signing][supports installation of unsigned add-ons]] (Firefox ESR, Nightly, Developer Edition, and Tor Browser do).
- Go to =about:config=, set =xpinstall.signatures.required= to =false=.
- Build the XPI by running =./make-firefox-xpi.sh= from the =extension= directory.
- Install it into your Firefox by going to =about:addons=, clicking the gear button, and selecting "Install Add-on from File" (or by doing =File > Open File= from the menu and selecting the XPI, or by drag-and-dropping the XPI into the browser window).
* Data format
[[https://datatracker.ietf.org/doc/html/rfc8949][CBOR (RFC8949)]] encoding of the following structure:

#+BEGIN_SRC
reqresV1 = [
    "WEBREQRES/1",
    source,
    protocol,
    [
        requestTimeStamp,
        requestMethod,
        requestURL,
        requestHeaders,
        isRequestComplete,
        requestBody,
    ],
    responseV1,
    endTimeStamp,
    optionalData,
]

responseV1 = null | [
    responseTimeStamp,
    responseStatusCode,
    responseReason,
    responseHeaders,
    isResponseComplete,
    responseBody,
]

optionalData = <map from str to anything>
#+END_SRC

- =source= is a short description of the data source, like =Firefox/102.0+pWebArc/0.1=;
- =optionalData= currently stores optional =origin_url= and =document_url= when different from browser's =Referer= request header;
- =responseV1= can be =null= when the request got no response, like when experiencing a network issue (dumping such request+response pairs is disabled by default).
* License
GPLv3+, some small library parts are MIT.
